module AI
abstract AbstractLayer

@inline function sigmoid(i::Matrix{Float64})
    @inbounds @fastmath begin
    exp_mi = similar(i)
    @simd for j in eachindex(exp_mi)
        expmi    = exp(-i[j])
        exp_mi[j]= 1.0/(expmi+1.0)
    end
    end
    exp_mi
end

@inline function dsigmoid(o::Matrix{Float64})
    dodi=similar(o)
    @inbounds @fastmath begin
    @simd for j in eachindex(dodi)
        oj     = o[j]
        dodit  = 1.0-oj
        dodi[j]= dodit*oj
    end
    end
    dodi
end

@inline function sigmoid!(o::Matrix{Float64},i::Matrix{Float64})
    @inbounds @fastmath begin
    @simd for j in eachindex(o)
        expmi    = exp(-i[j])
        o[j]= 1.0/(expmi+1.0)
    end
    end
    o
end

@inline function dsigmoid!(dodi::Matrix{Float64},o::Matrix{Float64})
    @inbounds @fastmath begin
    @simd for j in eachindex(dodi)
        oj     = o[j]
        dodit  = 1.0-oj
        dodi[j]= dodit*oj
    end
    end
    dodi
end

type HiddenLayer <: AbstractLayer
    I::Matrix{Float64}
    O::Matrix{Float64}
    dIdO::Matrix{Float64}
    function HiddenLayer(x::Matrix{Float64})
        y=sigmoid(x)
        z=dsigmoid(y)
        new(x,y,z)
    end
end

type TrainingSet
    IS::Matrix{Float64}
    OS::Matrix{Float64}
    function TrainingSet(x::Matrix{Float64}, y::Matrix{Float64})
        @assert size(x,2)==size(y,2)
        new(x,y)
    end
end

abstract AbstractNeuralNet

type SupervisedNet <: AbstractNeuralNet
    TS::TrainingSet
    OS::Matrix{Float64}
    HLS::Vector{HiddenLayer}
    Weights::Vector{Matrix{Float64}}
    dWLast::Vector{Matrix{Float64}}
end

function supervisednet(TS::TrainingSet,NNinHLs::Vector{Int})
    @inbounds @fastmath begin
    NHLs=length(NNinHLs)
    NI,NT=size(TS.IS)
    NO=size(TS.OS,1)

    Weights=Array(Matrix{Float64},NHLs+1)
    dWLast=Array(Matrix{Float64},NHLs+1)

    Weights[1]=rand(NNinHLs[1],NI)
    dWLast[1]=zeros(NNinHLs[1],NI)

    Weights[end]=rand(NO,NNinHLs[end])
    dWLast[end]=zeros(NO,NNinHLs[end])

    HLS=Array(HiddenLayer,NHLs)
    HLS[1]=HiddenLayer(Weights[1]*TS.IS)

    i=1
    while i<NHLs
        W=rand(NNinHLs[i+1],NNinHLs[i])
        HLS[i+1]=HiddenLayer(W*HLS[i].O)
        Weights[i+1]=W
        dWLast[i+1]=zeros(NNinHLs[i+1],NNinHLs[i])
        i+=1
    end
    OS=Weights[end]*HLS[end].O
    OS=sigmoid!(OS,OS)
    end
    SupervisedNet(TS,OS,HLS,Weights,dWLast)
end

function delta_end!(d_end,x,y)
    @inbounds @fastmath begin 
    dsigmoid!(d_end,y)
    @simd for i in eachindex(x)
        d_end[i] = (x[i]-y[i])*d_end[i]
    end
    end
    d_end
end

function Feedforward!(SN::SupervisedNet)
    @inbounds @fastmath begin
    IS = SN.TS.IS
    HLayers = SN.HLS          #Hidden Layers
    NHLs= length(HLayers)    #Number of Hidden Layers
    OS  = SN.OS
    Weights = SN.Weights
    A_mul_B!(HLayers[1].I,Weights[1],IS)
    sigmoid!(HLayers[1].O,HLayers[1].I)
    dsigmoid!(HLayers[1].dIdO,HLayers[1].O)
    for i=2:length(HLayers)
        A_mul_B!(HLayers[i].I,Weights[i],HLayers[i-1].O)
        sigmoid!(HLayers[i].O,HLayers[i].I)
        dsigmoid!(HLayers[i].dIdO,HLayers[i].O)
    end
    A_mul_B!(OS,Weights[end],HLayers[end].O)
    OS=sigmoid!(OS,OS)
    end
    SN
end

function Backpropagate!(SN::SupervisedNet, α=0.5, epoch=100)
    @inbounds @fastmath begin
    OS=SN.OS                #Output
    DOS=SN.TS.OS            #Desired Output
    IS =SN.TS.IS            #Input
    W=SN.Weights            #Weights
    dW=SN.dWLast            #Weights' Last Change
    HLayers=SN.HLS          #Hidden Layers
    NHLs=length(HLayers)    #Number of Hidden Layers
    NHLs += 1
    m = size(SN.TS.IS,2)
    α=α/m
    δ=Array(Matrix{Float64},NHLs)
    for i in eachindex(δ)
        δ[i]=Array(Float64,size(W[i],1),m)
    end
    while epoch>0
        i=NHLs
        delta_end!(δ[end],DOS,OS)
        while i>1
            Wₜ=W[i]
            dWₜ=dW[i]
            δₜ=δ[i]
            i-=1
            δₚ=At_mul_B!(δ[i],Wₜ, δₜ)
            inp=HLayers[i].O
            dIdO=HLayers[i].dIdO
            scale!(A_mul_Bt!(dWₜ,δₜ,inp),α)
            for j in eachindex(dWₜ)
                Wₜ[j] += dWₜ[j]
            end
            for k in eachindex(δₚ)
                δₚ[k]=δₚ[k]*dIdO[k]
            end
        end
        W1=W[1]
        δ1=δ[1]
        dW1=dW[1]
        scale!(A_mul_Bt!(dW1,δ1,IS),α)
        for j in eachindex(dW1)
            W1[j] += dW1[j]
        end

        Feedforward!(SN)
        epoch -= 1
    end
    end
    SN
end
Backpropagate!(SN::SupervisedNet, epoch::Int) = Backpropagate!(SN,0.5,epoch)
export TrainingSet, supervisednet, Backpropagate!, Feedforward!

end

#using AI

#Inputs = 1.0*rand(Bool, 12, 100)
#Outputs = 1.0*(Inputs[[2,5,8,11],:] .== 1.0)

#TSet=TrainingSet(Inputs,Outputs)

#NHLayers = [7; 6;]

#a=supervisednet(TSet,NHLayers)

#println(a.OS)
#println(a.Weights)

#Backpropagate!(a,1_000_000)

#println(a.OS)
#println(a.Weights)
