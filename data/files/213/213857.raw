## =============================================================================
## treepredict.jl - a re-written version of a decsition tree python
##                  by Toby Segaran's Collective inttelligence
## Copyright(C). masaru.charlie 2015. All rights reserved.
## =============================================================================
## import Base.import

my_data = readdlm("./data/decision_tree_example.txt", '\t')

## class definition
type DecisionNode
    col::Int
    value::ASCIIString
    tb::DecisionNode
    fb::DecisionNode
    results::ASCIIString

    function DecisionNode(col, value, tb, fb, results)
        new(col, value, tb, fb, results)
    end
end

## Divides a set on a specific column.
## Can handle numeric or nomnal values.
function divide_set(rows, column, value)

    idx_y = Array{Int}[]
    idx_n = Array{Int}[]

    if isa(value, Int) || isa(value, Float32)
        idx_y = find(x -> x >= value, rows[:, column])
        idx_n = find(x -> x <  value, rows[:, column])
    else
        idx_y = find(x -> x == value, rows[:, column])
        idx_n = find(x -> x != value, rows[:, column])
    end

    return (rows[idx_y, :], rows[idx_n, :])
end

## Create counts of possible results 
## (the last column of each row is the result.)
function unique_counts(rows)
    results = Dict{AbstractString, Int64}()
    (n_rows, n_cols) = size(rows)

    for i in 1:n_rows
        val = rows[i, n_cols] # value of the last column of each row
        if ! (val in keys(results))
            results[val] = 0
        end
        results[val] += 1
    end

    return results
end

## Probability that a randomly placed item wil be
## in the wrong category
function gini_impurity(rows)
    (n_rows, n_cols) = size(rows)
    counts = unique_counts(rows)
    imp = 0.0
    for k1 in keys(counts)
        p1 = float(counts[k1] / n_rows)
        for k2 in keys(counts)
            if k1 == k2
                continue
            end
            p2 = float(counts[k2] / n_rows)
            imp += p1 * p2
        end
    end

    return imp
end

## Entropy is the sum of p(x)*log(p(x)) across
## all the different possible results
function entropy(rows)
    (n_rows, n_cols) = size(rows)
    log_normalized = x -> log(x) / log(2)
    results = unique_counts(rows)

    ent = 0.0
    for k in keys(results)
        p = float(results[k] / n_rows)
        ent = ent - p * log_normalized(p)
    end

    return ent
end


## 
##
function buildtree(rows, scoref = entropy)
    (n_rows, n_cols) = size(rows)

    if n_rows == 0
        return DecisionNode()
    end

    current_score = scoref(rows)

    best_gain = 0.0
    best_criteria = nothing
    best_sets = nothing

    ## column_count = n_rows

    for col in 1:n_cols
        column_values = Dict{Any, Int64}()
        for val in rows[:, col]
            column_values[val] = 1
        end

        for val in keys(column_values)
            (set1, set2) = divide_set(rows, col, val)

            (n_rows1, n_cols1) = size(set1)
            (n_rows2, n_cols2) = size(set2)

            p = float(n_rows1) / n_rows
            gain = current_score - p * scoref(set1) - (1 - p) * scoref(set2)

            if gain > best_gain && n_rows1 > 0 && n_rows2 > 0
                best_gain = gain
                best_criteria = (col, val)
                best_sets = (set1, set2)
            end
        end
    end

    if best_gain > 0
        true_branch  = buildtree(best_sets[1])
        false_branch = buildtree(best_sets[2])

        return DecisionNode(col = best_criteria[1],
                            value = best_criteria[2],
                            tb = true_branch,
                            fb = false_branch)
    else
        return DecisionNode(results = unique_counts(rows))
    end
end

