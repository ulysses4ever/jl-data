#Caterpillar Tube Pricing Benchmark
#Julia Ver. 0.0.5 # data merged with 80% of the data

#Install Needed Libraries
#Pkg.add("BinDeps")
#Pkg.add("JSON")
#Pkg.add("DataFrames")
#Pkg.add("Gadfly")
#Pkg.add("XGBoost")

#Libraries, directories, options and extra functions----------------------
Pkg.update()

using JSON
using DataFrames
using Dates
using Gadfly
using XGBoost

#Read Settings file
cd(dirname(@__FILE__()))
directories = JSON.parsefile("SETTINGS.json")

dataDirectory = directories["dataDirectory"]
edaDirectory = directories["EDALoc"]

#Load data
train = readtable(dataDirectory * "train_set.csv")
test = readtable(dataDirectory * "test_set.csv")
test = test[:, 2:size(test, 2)]
test[:cost] = 0
train[:idx] = -[1:size(train, 1)]
test[:idx] = [1:size(test, 1)]
dataFull = vcat(train, test)

#Data Transformations
#Quote Dates to numeric columns
dataFull[:quote_date] = Date(dataFull[:quote_date], "y-m-d")

#New Columns
dataFull[:year] = year(dataFull[:quote_date])
dataFull[:month] = month(dataFull[:quote_date])
dataFull[:day] = day(dataFull[:quote_date])
dataFull[:dayWeek] = dayofweek(dataFull[:quote_date])

#Merge all dataframes
dataFull = join(dataFull, readtable(dataDirectory * "bill_of_materials.csv"),
                on = :tube_assembly_id, kind = :inner)
dataFull = join(dataFull, readtable(dataDirectory * "specs.csv"),
                on = :tube_assembly_id, kind = :inner)
dataFull = join(dataFull, readtable(dataDirectory * "tube.csv"),
                on = :tube_assembly_id, kind = :inner)

print("Current index order. First 6 elements: " * string(head(test[:idx])))
print("The size of the merged dataset is " * string(size(dataFull, 1)) * " rows and " *
      string(size(dataFull, 2)) * " columns.")

files2Merge = readdir(dataDirectory)[[2:7, 9:13]]

#Take the component data files and merge them together
for csv2read in files2Merge
  tempFile = readtable(dataDirectory * csv2read)
  originalNamesTempFile = names(tempFile)
  for i in [1:8]
    newColNames = [symbol(string(nameCol) * "_" * string(i)) for nameCol in originalNamesTempFile]
    names!(tempFile, newColNames)
    onJoinCol = symbol("component_id_" * string(i))
    dataFull = join(dataFull, tempFile, on = onJoinCol, kind = :left)
  end
end

print("THe size of the merged dataset is " * string(size(dataFull, 1)) * " rows and " *
      string(size(dataFull, 2)) * " columns.")

#Reduce Data Size; NA Removal from data frames
accumulatedNAInfo = colwise(isna, dataFull)
numberOfNAsPerColumn = map((x) -> sum(x) / length(x), accumulatedNAInfo)
namesAndTypes = hcat(names(dataFull), eltypes(dataFull), numberOfNAsPerColumn)

naThreshold = 0.95
validColumnsIdxs = bool(map(x -> x < naThreshold, numberOfNAsPerColumn))

dataFull = dataFull[:, [validColumnsIdxs]]

print("The size of the reduced dataset is " * string(size(dataFull, 1)) * " rows and " *
      string(size(dataFull, 2)) * " columns.")

accumulatedNAInfoReduced = colwise(isna, dataFull)
numberOfNAsPerColumnReduced = map((x) -> sum(x) / length(x), accumulatedNAInfoReduced)
namesAndTypesReduced = hcat(names(dataFull), eltypes(dataFull), numberOfNAsPerColumnReduced)

#EDA,
#EDA 1; Cost Histograms
#Cost histogram
plot(x = dataFull[dataFull[:idx].< 0, :cost], Geom.histogram,
     Guide.ylabel("Amount"), Guide.xlabel("Cost"))
#Log Cost histogram; useful for linear models and posible outlier detection
plot(x = dataFull[dataFull[:idx].< 0, :cost], Geom.histogram, Scale.x_log,
     Guide.ylabel("Amount"), Guide.xlabel("Cost in Log Scale"))

#EDA #2; tube cost vs. volume order
#This plot was based on the script found on:
#https://www.kaggle.com/timabram/caterpillar-tube-pricing/tube-pricing-data-exploration-v2
plot(x = dataFull[dataFull[:idx].< 0, :quantity], y = dataFull[dataFull[:idx].< 0, :cost],
     Geom.histogram, Geom.point, Scale.x_log, Scale.y_log,
     Geom.smooth(method = :loess, smoothing = 0.9),
     Guide.xlabel("Quantity Bought in Log Scale"), Guide.ylabel("Cost in Log Scale"))

#EDA #3: Number of NAs per column in merged dataframes
plot(x = 1:size(namesAndTypes, 1), y = namesAndTypes[:, 3], Geom.bar,
     Guide.xlabel("Column Names"), Guide.ylabel("Percentage of NAs in data"))

plot(x = 1:size(namesAndTypesReduced, 1), y = namesAndTypesReduced[:, 3], Geom.bar,
     Guide.xlabel("Column Names"), Guide.ylabel("Percentage of NAs in data"))


#Define Categorical and Numeric Rows
namesAndTypesReduced[:, 2] = map(x -> string(x), namesAndTypesReduced[:, 2])
categoricalColumns = names(dataFull)[namesAndTypesReduced[:, 2] .== "UTF8String"]
numericalColumns = names(dataFull)[namesAndTypesReduced[:, 2] .â‰  "UTF8String"]

#Clean Idx, cost and idx values from training features
cleanNumeric = !bool([bool(string(colSymbol) == "cost") | bool(string(colSymbol) == "quote_date") | bool(string(colSymbol) == "idx")
                      for colSymbol in numericalColumns])

numericalColumns = numericalColumns[cleanNumeric]

#Categorical Columns to Frequency
function categorical2frequency(dataVector)
  #This functions counts the occurences of a category and maps it back to the data provided
  #countmap is similar to the function "table()" in R, the only difference is that countmap() returns a dictionary
  vectorDict = countmap(dataVector)
  newVector = [vectorDict[dataPoint] for dataPoint in dataVector]
  return newVector
end

for singleCol in categoricalColumns
  dataFull[singleCol] = categorical2frequency(dataFull[singleCol])
end

#Remove NAs from data
for singleCol in numericalColumns
  dataFull[isna(dataFull[singleCol]) .== true, singleCol] = -999
end

#Remap the data to the original partitions
train = dataFull[dataFull[:idx].< 0, :]
test = dataFull[dataFull[:idx].> 0, :]
testIds = test[:idx]
print("Current index order. First 6 elements: " * string(head(train[:idx])))
print("Current index order. First 6 elements: " * string(head(test[:idx])))

#Shuffle train DataFrame
shuffledIdxs = shuffle([1:size(train, 1)])
train = train[shuffledIdxs, :]

#Transform DataFrame into Array{float32} (which is currently supported by xgboost in julia)
costTraining = convert(Array{Float32}, train[:, :cost])
costTrainingLog = log1p(convert(Array{Float32}, train[:, :cost]))
trainArray = convert(Array{Float32}, train[:, vcat(categoricalColumns, numericalColumns)])
testArray = convert(Array{Float32}, test[:, vcat(categoricalColumns, numericalColumns)])

#XGBOOST
#Create Binary Matrices
dtrain = DMatrix(trainArray, label = costTrainingLog)
dtest = DMatrix(testArray)

#Define the evaluation function (xgboost currently doesn't support RMSLE)
function evalRMSLE(preds::Array{Float32, 1}, dtrain::DMatrix)
  labels = get_info(dtrain, "label")
  # return a pair metric_name, result
  differenceLogs = log(1 + preds) - log(1 + labels)
  RMSLEError = sqrt(1/length(differenceLogs) * sum([x ^ 2 for x in differenceLogs]))
  return ("RMSLE", RMSLEError)
end

#5fold Cross-Validation
nfold = 5
num_round = 1000
param = ["max_depth" => 10, "objective" => "reg:linear", "silent" => 1]

#nfold_cv(dtrain, num_round, nfold, param = param, seed = 0)  #This takes too long, the evaluation function has to be further optimized

#Model Training
#with RMSLE Evaluation
#XGBoostModel = xgboost(dtrain, num_round, param = param, feval = evalRMSLE)
#without RMSLE Evaluation
XGBoostModel = xgboost(dtrain, num_round, param = param)

#Model Evaluation
dump_model(XGBoostModel, "dump.nice.txt")

#Predictions using test data
preds = exp(predict(XGBoostModel, dtest)) - 1

#Write Results
sampleSubmission = readtable(dataDirectory * "sample_submission.csv")
sampleSubmission[:id] = testIds
sampleSubmission[:cost] = preds

writetable("juliaXGBoostII.csv", sampleSubmission)

