#
# LNNA: Living Neural Network Analyzer 
# Copyright (C) 2015  Jimmy Dubuisson <jimmy.dubuisson@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#

using Logging

@Logging.configure(level=DEBUG)

function get_H_entry(s::Int64,t::Int64,alphas::Array{Float64,1},betas::Array{Float64,2},gammas::Array{Float64,2})
	n = length(alphas)
	s_digits = digits(s,2,n)
	t_digits = digits(t,2,n)

	q = float64(0)
	# @debug(s,":",t)

	for i in 1:n
		q += alphas[i]*t_digits[i]
	end
	for i in 1:n
		for j in 1:n
			q += betas[i,j]*t_digits[i]*t_digits[j]
			q += gammas[i,j]*s_digits[i]*t_digits[j]
		end
	end

	return exp(q)
end

# initialize matrix H
function init_H(n::Int64,alpha_min::Float64,alpha_max::Float64,beta_min::Float64,beta_max::Float64,gamma_min::Float64,gamma_max::Float64)
	# inititalize alpha array
	alphas = rand(n)
	alphas = float64([alphas[i]*(alpha_max-alpha_min)+alpha_min for i in 1:n])

	# initialize matrix betas
	betas = rand(n,n)
	betas = float64([betas[i,j]*(beta_max-beta_min)-beta_min for i in 1:n, j in 1:n])
	betas = [i>j?betas[i,j]:betas[j,i] for i in 1:n, j in 1:n]
	
	# initialize matrix gammas
	gammas = rand(n,n)
	gammas = float64([gammas[i,j]*(gamma_max-gamma_min)-gamma_min for i in 1:n, j in 1:n])

	# initialize matrix H
	H = zeros(2^n,2^n)

	for x in 1:2^n
		for y in 1:2^n
			H[x,y] = get_H_entry(x-1,y-1,alphas,betas,gammas)
		end
	end

	return H
end

function get_principal_eigenvector(H)
	 F = eigfact(H)
	 return F.values[1],F.vectors[:,1]
end

function normalize_H(H,lambda,R)
	N = size(H)[1]
	for x in 1:N
		for y in 1:N
			H[x,y] = (H[x,y]*R[y])/(R[x]*lambda)
		end
	end
end

function get_stationary_distribution(L,R)
	N = length(L)
	dp = dot(L,R)
	return float64([(L[i]*R[i])/dp for i in 1:N])
end

function get_entropy(P,p)
	N = size(P)[1]
	Ep = 0.
	for x in 1:N
		for y in 1:N
			nt = p[x]*P[x,y]
			dt = p[y]*P[y,x]
			Ep += (nt-dt)*log(nt/dt)
		end
	end
	return Ep/2
end

function load_data(filename::String,bin_length::Float64)
	f = open(filename, "r")
	lines = readlines(f)
	close(f)
	# nid -> array of bids
	Dn = Dict{Int64,Array{Int64,1}}()
	# bid -> array of nids
	Db = Dict{Int64,Array{Int64,1}}()
	for l in lines
		ts,ns = split(l,'\t')
		t = float64(ts)
		n = int64(ns)
		bid = int64(floor(t/bin_length)+1)
		# @debug("$bid : $n")
		if haskey(Dn,n)
			push!(Dn[n],bid)
		else
			Dn[n] = [bid]
		end
		if haskey(Db,bid)
			push!(Db[bid],n)
		else
			Db[bid] = [n]
		end
	end
	return Dn,Db
end

# load dataset
Dn, Db = load_data(ARGS[1],20.)

# sort Dn dictionary by values length
sDn = sort(collect(Dn), by = tuple -> length(last(tuple)), rev=true)

Dc = Dict{String,Int64}()

# look at empty bins
mDb = maximum(keys(Db))
Dc[""] = mDb - length(values(Db))

for v in values(Db)
	sort!(v)
	sv = "$v"[2:(end-1)]
	#hsv = hash(sv)
	if haskey(Dc,sv)
		Dc[sv] = Dc[sv] + 1
	else
		Dc[sv] = 1
	end
end

# sort Dc dictionary by values
sDc = sort(collect(Dc), by = tuple -> last(tuple), rev=true)[1:10]
println(sDc)

quit()

H = init_H(3, 0.0,1.0, 0.0,1.0, 0.0,1.0)
println(H)

lambda,R = get_principal_eigenvector(H)
lambda,L = get_principal_eigenvector(H')

normalize_H(H,lambda,R)
println(H)

p = get_stationary_distribution(L,R)
println(p)

Ep = get_entropy(H,p)
println(Ep)
