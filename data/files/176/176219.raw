# NOTE: this is part of the @cuda test set, but needs to be top-level
module KernelModule
    export do_more_nothing
    @target ptx do_more_nothing() = return nothing
end

@testset "CUDA.jl native" begin

dev = CuDevice(0)
ctx = CuContext(dev)


@testset "code generation" begin
    @testset "LLVM IR" begin
        @target ptx foo() = return nothing
        ir = sprint(io->code_llvm(io, foo, (),
                                  #=strip_ir_metadata=#true, #=dump_module=#true))

        # module should only contain our function + a generic call wrapper
        @test length(matchall(r"define .+", ir)) == 2
        @test ismatch(r"define void @julia_.+_foo_.+\(\) #0 \{", ir)
        @test ismatch(r"define %jl_value_t\* @jlcall_", ir)
        # module should be created for the PTX back-end
        @test contains(ir, "!\"Julia Codegen Target\", !\"ptx\"")
        # function should be generated by the PTX back-end
        @test ismatch(r"attributes #0 = \{.+\"jl_cgtarget\"=\"ptx\".+\}", ir)
        # GC frame ref should have been optimized away
        # NOTE: disabled now that the module only gets optimized later on,
        #       which means the frame ref is still part of code_llvm
        #@test !contains(ir, "jl_get_ptls_states")
    end

    @testset "PTX assembly" begin
        # TODO: PTX assembly generation / code_native
        # -> test if foo and bar doesn't end up in same PTX module

        # TODO: assert .entry
        # TODO: assert devfun non .entry
    end

    @testset "exceptions" begin
        @target ptx function throw_exception()
            throw(DivideError())
        end
        ir = sprint(io->code_llvm(io, throw_exception, ()))

        # exceptions should get lowered to a plain trap...
        @test contains(ir, "llvm.trap")
        # not a jl_throw referencing a jl_value_t representing the exception
        @test !contains(ir, "jl_value_t")
        @test !contains(ir, "jl_throw")
    end

    # delayed binding lookup (due to noexisting global)
    let
        @target ptx foo() = nonexisting
        @test_throws ErrorException code_native(DevNull, foo, ())
    end

    # generic call to nonexisting function
    let
        @target ptx foo() = nonexisting()
        @test_throws ErrorException code_native(DevNull, foo, ())
    end

    # cannot call PTX functions
    let
        @target ptx foo() = return nothing
        @test_throws ErrorException foo()
    end

    # bug: generate code twice for the same kernel (jl_to_ptx wasn't idempotent)
    let
        @target ptx foo() = return nothing
        code_native(DevNull, foo, ())
        code_native(DevNull, foo, ())
    end

    # bug: depending on a child function from multiple parents resulted in
    #      the child only being present once
    # NOTE: disabled because of #15276 / #15967
    let
        @target ptx @noinline function child()
            return 0
        end

        @target ptx function parent1(arr::Ptr{Int64})
            i = child()
            unsafe_store!(arr, i, i)
            return nothing
        end
        # asm = sprint(io->code_native(io, parent1, (Ptr{Int64},)))
        # @test ismatch(r".visible .func .+ julia_child", asm)


        @target ptx function parent2(arr::Ptr{Int64})
            i = child()+1
            unsafe_store!(arr, i, i)

            return nothing
        end
        # asm = sprint(io->code_native(io, parent2, (Ptr{Int64},)))
        # @test ismatch(r".visible .func .+ julia_child", asm)
    end

    # bug: similar, but slightly different issue as above
    #      in the case of two child functions
    # NOTE: disabled because of #15276 / #15967
    let
        @target ptx @noinline function child1()
            return 0
        end

        @target ptx @noinline function child2()
            return 0
        end

        @target ptx function parent1(arry::Ptr{Int64})
            i = child1() + child2()
            unsafe_store!(arry, i, i)

            return nothing
        end
        # asm = sprint(io->code_native(io, parent1, (Ptr{Int64},)))


        @target ptx function parent2(arry::Ptr{Int64})
            i = child1() + child2()
            unsafe_store!(arry, i, i+1)

            return nothing
        end
        # asm = sprint(io->code_native(io, parent2, (Ptr{Int64},)))
    end
end


@testset "@cuda" begin
    @target ptx do_nothing() = return nothing

    @test_throws UndefVarError @cuda (1, 1) undefined_kernel()

    @testset "kernel dims" begin
        @test_throws ArgumentError @cuda (0, 0) do_nothing()
        @cuda (1, 1) do_nothing()
    end

    @testset "external kernel" begin
        @cuda (1, 1) KernelModule.do_more_nothing()
        @eval begin
            using KernelModule
            @cuda (1, 1) do_more_nothing()
        end
    end

    @testset "return values" begin
        @target ptx retint() = return 1
        @test_throws ErrorException @cuda (1, 1) retint()

        # TODO: test whether child functions can return values
        #       (testing blocked by #15276 / #15967)
    end

    @testset "argument passing" begin
        dims = (16, 16)
        len = prod(dims)

        @target ptx function array_copy(input::CuDeviceArray{Float32},
                                        output::CuDeviceArray{Float32})
            i = blockIdx().x +  (threadIdx().x-1) * gridDim().x
            output[i] = input[i]

            return nothing
        end

        @testset "manual allocation" begin
            input = round(rand(Float32, dims) * 100)

            input_dev = CuArray(input)
            output_dev = CuArray(Float32, dims)

            @cuda (len, 1) array_copy(input_dev, output_dev)
            output = to_host(output_dev)
            @test_approx_eq input output

            free(input_dev)
            free(output_dev)
        end

        # Copy non-bit array
        @test_throws ArgumentError begin
            # Something that's certainly not a bit type
            f =  x -> x*x
            input = [f for i=1:10]
            cu_input = CuArray(input)
        end

        # CuArray with not-bit elements
        let
            @test_throws ArgumentError CuArray(Function, 10)
            @test_throws ArgumentError CuArray(Function, (10, 10))
        end

        # cu mem tests
        let
            @test_throws ArgumentError CUDAnative.cualloc(Function, 10)

            dev_array = CuArray(Int32, 10)
            CUDAnative.cumemset(dev_array.ptr, UInt32(0), 10)
            host_array = to_host(dev_array)

            for i in host_array
                @assert i == 0 "Memset failed on element $i"
            end

            CUDAnative.free(dev_array)
        end

        @target ptx function array_lastvalue(a::CuDeviceArray{Float32},
                                             x::CuDeviceArray{Float32})
            i = blockIdx().x +  (threadIdx().x-1) * gridDim().x
            max = gridDim().x * blockDim().x
            if i == max
                x[1] = a[i]
            end

            return nothing
        end

        # scalar through single-value array
        let
            arr = round(rand(Float32, dims) * 100)
            val = Float32[0]

            arr_dev = CuArray(arr)
            val_dev = CuArray(val)

            @cuda (len, 1) array_lastvalue(arr_dev, val_dev)
            @test_approx_eq arr[dims...] to_host(val_dev)[1]
        end

        @target ptx @noinline function array_lastvalue_devfun(a::CuDeviceArray{Float32},
                                                              x::CuDeviceArray{Float32})
            i = blockIdx().x +  (threadIdx().x-1) * gridDim().x
            max = gridDim().x * blockDim().x
            if i == max
                x[1] = lastvalue_devfun(a, i)
            end

            return nothing
        end

        @target ptx function lastvalue_devfun(a::CuDeviceArray{Float32}, i)
            return a[i]
        end

        # same, but using a device function
        # NOTE: disabled because of #15276 / #15967
        let
            arr = round(rand(Float32, dims) * 100)
            val = Float32[0]

            arr_dev = CuArray(arr)
            val_dev = CuArray(val)

            # @cuda (len, 1) array_lastvalue_devfun(arr_dev, val_dev)
            # @test_approx_eq arr[dims...] to_host(val_dev)[1]
        end
    end

    @testset "intrinsics" begin
        buf = CuArray(Float32, 1)

        @target ptx function kernel_log10(a::CuDeviceArray{Float32}, i::Float32)
            a[1] = CUDAnative.log10(i)
            return nothing
        end

        @cuda (1, 1) kernel_log10(buf, Float32(100))
        val = to_host(buf)
        @test_approx_eq val[1] 2.0

        free(buf)
    end
end

@testset "shared memory" begin
    @target ptx function kernel_reverse{T}(d::CuDeviceArray{T}, n)
        t = threadIdx().x
        tr = n-t+1

        s = cuSharedMem(T)
        s[t] = d[t]
        sync_threads()
        d[t] = s[tr]

        return nothing
    end

    n = 1024
    types = [Int64, Float32, Float64]

    for T in types
        a = rand(T, n)
        d_a = CuArray(a)

        @cuda (1, n, n*sizeof(T)) kernel_reverse(d_a, n)

        @assert reverse(a) == to_host(d_a)
    end
end


destroy(ctx)

end
