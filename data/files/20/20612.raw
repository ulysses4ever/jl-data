# Preconditioners
#  * Empty preconditioner
cg_precondfwd(out::Array, P::Nothing, A::Array) = copy!(out, A)
cg_precondfwddot(A::Array, P::Nothing, B::Array) = dot(A, B)
cg_precondinvdot(A::Array, P::Nothing, B::Array) = dot(A, B)

# Diagonal preconditioner
function cg_precondfwd(out::Array, p::Vector, A::Array)
    for i in 1:length(A)
        out[i] = p[i] * A[i]
    end
    return out
end
function cg_precondfwddot{T}(A::Array{T}, p::Vector, B::Array)
    s = zero(T)
    for i in 1:length(A)
        s += A[i] * p[i] * B[i]
    end
    return s
end
function cg_precondinvdot{T}(A::Array{T}, p::Vector, B::Array)
    s = zero(T)
    for i in 1:length(A)
        s += A[i] * B[i] / p[i]
    end
    return s
end

#
# Conjugate gradient
#
# This is an independent implementation of:
#   W. W. Hager and H. Zhang (2006) Algorithm 851: CG_DESCENT, a
#     conjugate gradient method with guaranteed descent. ACM
#     Transactions on Mathematical Software 32: 113â€“137.
#
# Code comments such as "HZ, stage X" or "HZ, eqs Y" are with
# reference to a particular point in this paper.
#
# Several aspects of the following have also been incorporated:
#   W. W. Hager and H. Zhang (2012) The limited memory conjugate
#     gradient method.
#
# This paper will be denoted HZ2012 below.
#
# There are some modifications and/or extensions from what's in the
# paper (these may or may not be extensions of the cg_descent code
# that can be downloaded from Hager's site; his code has undergone
# numerous revisions since publication of the paper):
#
# cgdescent: the termination condition employs a "unit-correct"
#   expression rather than a condition on gradient
#   components---whether this is a good or bad idea will require
#   additional experience, but preliminary evidence seems to suggest
#   that it makes "reasonable" choices over a wider range of problem
#   types.
#
# linesearch: the Wolfe conditions are checked only after alpha is
#   generated either by quadratic interpolation or secant
#   interpolation, not when alpha is generated by bisection or
#   expansion. This increases the likelihood that alpha will be a
#   good approximation of the minimum.
#
# linesearch: In step I2, we multiply by psi2 only if the convexity
#   test failed, not if the function-value test failed. This
#   prevents one from going uphill further when you already know
#   you're already higher than the point at alpha=0.
#
# both: checks for Inf/NaN function values
#
# both: support maximum value of alpha (equivalently, c). This
#   facilitates using these routines for constrained minimization
#   when you can calculate the distance along the path to the
#   disallowed region. (When you can't easily calculate that
#   distance, it can still be handled by returning Inf/NaN for
#   exterior points. It's just more efficient if you know the
#   maximum, because you don't have to test values that won't
#   work.) The maximum should be specified as the largest value for
#   which a finite value will be returned.  See, e.g., limits_box
#   below.  The default value for alphamax is Inf. See alphamaxfunc
#   for cgdescent and alphamax for linesearch_hz.

function cg_trace!(tr::OptimizationTrace,
                   x::Vector,
                   f_x::Real,
                   gr::Vector,
                   alpha::Real,
                   iteration::Integer,
                   store_trace::Bool,
                   show_trace::Bool)
    dt = Dict()
    dt["g(x)"] = copy(gr)
    dt["Current step size"] = alpha
    dt["Maximum component of g(x)"] = norm(gr, Inf)
    os = OptimizationState(copy(x), f_x, iteration, dt)
    if store_trace
        push!(tr, os)
    end
    if show_trace
        println(os)
    end
end

function cg{T}(df::Union(DifferentiableFunction,
                         TwiceDifferentiableFunction),
               initial_x::Array{T};
               tolerance::Real = eps(T)^(2/3),
               iterations::Integer = 1_000,
               store_trace::Bool = false,
               show_trace::Bool = false,
               linesearch!::Function = hz_linesearch!,
               eta::Real = 0.4,
               P::Any = nothing,
               precondprep::Function = (P, x) -> nothing)

    # Maintain current state in x
    x = copy(initial_x)

    # Count the total number of iterations
    iteration = 0

    # Track calls to function and gradient
    f_calls = 0
    g_calls = 0

    # Count number of parameters
    n = length(x)

    # Maintain current gradient in gr and previous gradient in gr_previous
    gr = similar(x)
    gr_previous = similar(x)

    # Maintain the preconditioned gradient in pgr
    pgr = similar(x)

    # The current search direction
    s = similar(x)

    # Buffers for use in line search
    x_ls = similar(x)
    gr_ls = similar(x)

    # Intermediate value in CG calculation
    y = similar(x)

    # Store f(x) in f_x
    f_x = df.fg!(x, gr)
    f_calls += 1
    g_calls += 1
    copy!(gr_previous, gr)

    # Store the history of function values
    f_values = Array(T, iterations + 1)
    fill!(f_values, nan(T))
    f_values[iteration + 1] = f_x

    # Keep track of step-sizes
    alpha = alphainit(1.0, x, gr, f_x)

    # TODO: How should this flag be set?
    mayterminate = false

    # Maintain a cache for line search results
    lsr = LineSearchResults(T)

    # Trace the history of states visited
    tr = OptimizationTrace()
    tracing = store_trace || show_trace
    if tracing
        cg_trace!(tr, x, f_x, gr, alpha,
                  iteration, store_trace, show_trace)
    end

    # Output messages
    if !isfinite(f_x)
        error("Must have finite starting value")
    end
    if !all(isfinite(gr))
        @show gr
        @show find(!isfinite(gr))
        error("Gradient must have all finite values at starting point")
    end

    # Determine the intial search direction
    precondprep(P, x)
    cg_precondfwd(s, P, gr)
    for i in 1:n
        s[i] = -s[i]
    end

    # Iterate until convergence
    f_converged = false
    gr_converged = false
    converged = false
    while !converged && iteration < iterations
        # Increment the number of steps we've had to perform
        iteration += 1

        # Reset the search direction if it becomes corrupted
        dphi0 = dot(gr, s)
        if dphi0 >= 0
            for i in 1:n
                s[i] = -gr[i]
            end
            dphi0 = dot(gr, s)
            if dphi0 < 0
                break
            end
        end

        # Refresh the line search cache
        clear!(lsr)
        push!(lsr, zero(T), f_x, dphi0)

        # Pick the initial step size (HZ #I1-I2)
        alpha, mayterminate, f_update, g_update =
          alphatry(alpha, df, x, s, x_ls, gr_ls, lsr)
        f_calls += f_update
        g_calls += g_update

        # Determine the distance of movement along the search line
        alpha, f_update, g_update =
          linesearch!(df, x, s, x_ls, gr_ls, lsr, alpha, mayterminate)
        f_calls += f_update
        g_calls += g_update

        # Update current position
        for i in 1:n
            x[i] = x[i] + alpha * s[i]
        end

        # Maintain a record of the previous gradient
        copy!(gr_previous, gr)

        # Update the function value and gradient
        f_x = df.fg!(x, gr)
        f_calls += 1
        g_calls += 1
        f_values[iteration + 1] = f_x

        # Check sanity of function and gradient
        if !isfinite(f_x)
            error("Function must finite function values")
        end

        # Determine the next search direction using HZ's CG rule
        #  Calculate the beta factor (HZ2012)
        precondprep(P, x)
        dPd = cg_precondinvdot(s, P, s)
        etak::T = eta * dot(s, gr_previous) / dPd
        for i in 1:n
            y[i] = gr[i] - gr_previous[i]
        end
        ydots = dot(y, s)
        cg_precondfwd(pgr, P, gr)
        betak = (dot(y, pgr) - cg_precondfwddot(y, P, y) * dot(gr, s) / ydots) / ydots
        beta = max(betak, etak)
        for i in 1:n
            s[i] = beta * s[i] - pgr[i]
        end

        # Assess convergence
        if norm(gr, Inf) < tolerance
            gr_converged = true
        end
        if abs(f_values[iteration + 1] - f_values[iteration]) < 1e-32
            f_converged = true
        end
        converged = gr_converged || f_converged

        # Show trace
        if tracing
            cg_trace!(tr, x, f_x, gr, alpha,
                      iteration, store_trace, show_trace)
        end
    end

    OptimizationResults("Conjugate Gradient",
                        initial_x,
                        x,
                        f_x,
                        iteration,
                        f_converged,
                        gr_converged,
                        tr,
                        f_calls,
                        g_calls,
                        f_values[1:(iteration + 1)])
end
