### WORK IN PROGRESS!!!

# TODO: update doc.

"""
charlm.jl: Knet8 version (c) Emre Yolcu, Deniz Yuret, 2016

This example implements an LSTM network for training character-level
language models. It takes as input a text file and trains the network
to predict the next character in a sequence. It can then be used to
generate a sample that resembles the original text.  Based on [The
Unreasonable Effectiveness of Recurrent Neural
Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness) from
the Andrej Karpathy blog.

To train the network, do `include("charlm.jl")` and run
`CharLM.train()`. Initial parameters can be provided as an optional argument to
`train`. Required form of the parameters can be understood by looking at the
definition of `weights` below. If not provided, default parameters are created
using `weights` with the appropriate arguments passed from `train`.

    `train` accepts the following keyword arguments:

    - `datasrc`: Path to a file, or a URL. "The Complete Works of Shakespeare"
    from Project Gutenberg will be used by default.
    - `char_limit`: Maximum number of characters to read from `datasrc`
    - `epochs`: Number of epochs
    - `lr_init`: Initial learning rate
    - `lr_decay`: Learning rate decay
    - `decay_after`: Epoch after which to start decaying learning rate
    - `embedding_size`: Size of the embedding vector
    - `hidden_size`: Size of the LSTM internal state
    - `batch_size`: Number of sequences to train on in parallel
    - `sequence_length`: Number of steps to unroll the network for
        - `gclip`: Value to clip the gradient norm at
        - `pdrop`: Dropout probability
        - `seed`: Seed for the random number generator

            At the end of each epoch, the cross entropy loss for the training data, the
                learning rate, and the time taken by the epoch to complete are
                printed. Optimized parameters and an array that maps integer indices to their
                characters are returned at the end of training. Then, a sample text can be
        generated by running `CharLM.generate(w, rv, n)` where `w` is the dictionary of
        parameters, `rv` is the mapping of indices to characters, and `n` is the number
        of characters to be generated.
        """
foo=0 # module CharLM

using Knet,AutoGrad,ArgParse,Compat
using Base.LinAlg: axpy!


function main(args=ARGS)
    global model,text,data,vocab,o
    s = ArgParseSettings()
    s.description="charlm.jl (c) Emre Yolcu, Deniz Yuret, 2016. Character level language model based on http://karpathy.github.io/2015/05/21/rnn-effectiveness."
    s.exc_handler=ArgParse.debug_handler
    @add_arg_table s begin
        ("--datafiles"; nargs='+'; help="If provided, use first file for training, second for dev, others for test.")
        ("--loadfile"; help="Initialize model from file")
        ("--savefile"; help="Save final model to file")
        ("--bestfile"; help="Save best model to file")
        ("--generate"; arg_type=Int; default=0; help="If non-zero generate given number of characters.")
        ("--nlayer"; arg_type=Int; default=1; help="Number of LSTM layers.")
        ("--hidden"; arg_type=Int; default=256; help="Size of the LSTM internal state.")
        ("--embedding"; arg_type=Int; default=256; help="Size of the embedding vector.")
        ("--epochs"; arg_type=Int; default=10; help="Number of epochs for training.")
        ("--batchsize"; arg_type=Int; default=128; help="Number of sequences to train on in parallel.")
        ("--seqlength"; arg_type=Int; default=100; help="Number of steps to unroll the network for.")
        ("--decay"; arg_type=Float64; default=0.9; help="Learning rate decay.")
        ("--lr"; arg_type=Float64; default=1.0; help="Initial learning rate.")
        ("--gclip"; arg_type=Float64; default=5.0; help="Value to clip the gradient norm at.")
        ("--dropout"; arg_type=Float64; default=0.0; help="Dropout probability.")
        ("--seed"; arg_type=Int; default=42; help="Random number seed.")
        # TODO: does this override if we are loading from file?
        # TODO: JLD must represent KnetArray
        ("--atype"; default=(gpu()>=0 ? "KnetArray{Float32}" : "Array{Float32}"); help="array type: Array for cpu, KnetArray for gpu")
    end
    isa(args, AbstractString) && (args=split(args))
    o = parse_args(args, s; as_symbols=true)
    println("opts=",[(k,v) for (k,v) in o]...)
    o[:seed] > 0 && srand(o[:seed])
    o[:atype] = eval(parse(o[:atype]))

    # we initialize a model from loadfile, train using datafiles (both optional).
    # if the user specifies neither, train a model using shakespeare.
    isempty(o[:datafiles]) && o[:loadfile]==nothing && push!(o[:datafiles],shakespeare())

    # read text and report lengths
    text = map((@compat read), o[:datafiles]) # TODO: readstring gets confused with BOM marker in pg100.txt
    !isempty(text) && info("Chars read: $(map((f,c)->(basename(f),length(c)),o[:datafiles],text))")

    # vocab (char_to_index) comes from the initial model if there is one, otherwise from the datafiles.
    # if there is an initial model make sure the data has no new vocab
    if o[:loadfile]==nothing
        vocab = Dict{Char,Int}()
        for t in text, c in t; get!(vocab, c, 1+length(vocab)); end
        model = weights(length(vocab), o) # TODO: only send necessary o options
    else
        vocab = load(o[:loadfile], "vocab") 
        for t in text, c in t; haskey(vocab, c) || error("Unknown char $c"); end
        model = load(o[:loadfile], "model") # TODO: possibly convert to atype
    end

    if !isempty(text)
        data = map(t->minibatch(t, vocab, o), text)
        println((0,map(d->loss(model,d,initstate(o)), data)...))
        for epoch=1:o[:epochs]           # TODO: model save
            @time train(model,data[1]; slen=o[:seqlength], lr=o[:lr]) # TODO: gclip, lrdecay, gcheck, profile
            @time println((epoch,map(d->loss(model,d,initstate(o)), data)...))
        end
    end
    if o[:generate] > 0
        generate(model, vocab, o)
    end
end


function shakespeare()
    file = Pkg.dir("Knet/data/100.txt")
    if !isfile(file)
        info("Downloading 'The Complete Works of William Shakespeare'")
        url = "http://www.gutenberg.org/files/100/100.txt"
        download(url,file)
    end
    return file
end


function minibatch(chars, char_to_index, o)
    nbatch = div(length(chars), o[:batchsize])
    vocab_size = length(char_to_index)
    data = Any[]
    for i = 1:nbatch
        d = zeros(Float32, (vocab_size, o[:batchsize]))
        for j = 1:o[:batchsize]
            d[char_to_index[chars[i + nbatch * (j - 1)]], j] = 1
        end
        push!(data, convert(o[:atype], d))
    end
    return data
end

function train(w, x; slen=100, lr=1.0)
    state = initstate(o)
    for t = 1:slen:length(x)-slen
        r = t:t+slen-1
        g = lossgradient(w, x, state; range=r)
        for k in keys(g)
            w[k] -= lr * g[k]
            # TODO: implement weight decay
            # TODO: implement gradient clip
        end
    end
end

# Given parameters w, sequence x, and hidden-cell pair state, returns
# loss and modifies state in-place. # TODO: dropout
function loss(w, x, state; range=1:length(x)-1)
    (h,c) = state
    logp = 0.0; xcnt = 0
    for t in range
        xt = w[:W_embedding] * x[t]
        (h,c) = lstm(w, xt, h, c)
        ypred = w[:W_predict] * h .+ w[:b_predict]
        ynorm = ypred .- log(sum(exp(ypred),1))
        logp += sum(x[t+1] .* ynorm)
        xcnt += size(ynorm,2)
    end
    state[1]=h; state[2]=c
    return -logp/xcnt
end

lossgradient = grad(loss)

# TODO: implement vcat for KnetArray and try various concat versions for efficiency.
# TODO: profile this.
# TODO: gradcheck!!!

function lstm(w, input, hidden, cell)
    ingate  = sigm(w[:Wx_ingate]  * input .+ w[:Wh_ingate] * hidden .+ w[:b_ingate]) # in fact we can probably combine these four operations into one
    forget  = sigm(w[:Wx_forget]  * input .+ w[:Wh_forget] * hidden .+ w[:b_forget]) # then use indexing, or (better) subarrays to get individual gates
    outgate = sigm(w[:Wx_outgate] * input .+ w[:Wh_outgate] * hidden .+ w[:b_outgate])
    change  = tanh(w[:Wx_change]  * input .+ w[:Wh_change] * hidden .+ w[:b_change])
    cell    = cell .* forget + ingate .* change
    hidden  = outgate .* tanh(cell)
    return hidden, cell
end

sigm(x) = 1 ./ (1 + exp(-x))
@primitive sigm(x::Array),dy,y  (dy .* y .* (1 - y))

function initstate(o)
    hidden = convert(o[:atype], zeros(o[:hidden], o[:batchsize]))
    cell   = convert(o[:atype], zeros(o[:hidden], o[:batchsize]))
    Any[hidden, cell]
end

function weights(vocabsize,o)
    w = Dict()
    for gate in (:ingate, :forget, :outgate, :change)
        w[Symbol("Wx_$gate")] = xavier(o[:hidden], o[:embedding])
        w[Symbol("Wh_$gate")] = xavier(o[:hidden], o[:hidden])
        w[Symbol("b_$gate")] = (gate == :forget ? ones : zeros)(Float32, (o[:hidden], 1))
    end
    w[:W_embedding] = xavier(o[:embedding], vocabsize)
    w[:W_predict]   = xavier(vocabsize, o[:hidden])
    w[:b_predict]   = zeros(Float32, (vocabsize, 1))
    for (k,v) in w
        w[k] = convert(o[:atype], v)
    end
    return w
end

function xavier(a...)
    w = rand(a...)
     # The old implementation was not right for fully connected layers:
     # (fanin = length(y) / (size(y)[end]); scale = sqrt(3 / fanin); axpb!(rand!(y); a=2*scale, b=-scale)) :
    if ndims(w) < 2
        error("ndims=$(ndims(w)) in xavier")
    elseif ndims(w) == 2
        fanout = size(w,1)
        fanin = size(w,2)
    else
        fanout = size(w, ndims(w)) # Caffe disagrees: http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1XavierFiller.html#details
        fanin = div(length(w), fanout)
    end
    # See: http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf
    s = sqrt(2 / (fanin + fanout))
    w = 2s*w-s
end

!isinteractive() && main(ARGS)

# end  # module


### DEAD CODE:

# function predict(w, hidden)
#     output = w[:W_predict] * hidden .+ w[:b_predict]
#     return output .- log(sum(exp(output), 1))
# end

# function dropout(x, pdrop)
#     return x .* (rand(size(x)) .< (1 - pdrop)) / (1 - pdrop)
# end

# function loss(w, inputs, targets; kwargs...)
#     outputs = forw(w, inputs; kwargs...)[1]
#     n = length(inputs)
#     z = 0.0
#     for t = 1:n
#         z += sum(outputs[t] .* targets[t])
#     end
#     return -z / size(inputs[1], 2)
# end

# function gnorm(g)
#     return mapreduce(vecnorm, +, 0, values(g))
# end

# function train(w=nothing; datasrc=nothing, char_limit=0, epochs=1, lr_init=1.0,
#                lr_decay=0.95, decay_after=10, embedding_size=128,
#                hidden_size=256, batch_size=50, sequence_length=50, gclip=5.0,
#                pdrop=0, seed=0)
#     seed > -1 && srand(seed)
#     data, chars, char_to_index, index_to_char = loaddata(datasrc, batch_size, char_limit)
#     vocab_size = length(char_to_index)
#     if w == nothing
#         w = weights(; input_size=vocab_size, output_size=vocab_size,
#                     embedding_size=embedding_size, hidden_size=hidden_size,
#                     batch_size=batch_size)
#     end
#     gradfun = grad(loss)

#     for epoch = 1:epochs
#         start_time = time()
#         targets = Any[]
#         inputs = Any[]
#         loss_count = zeros(2)
#         lr = lr_init * lr_decay^max(0, epoch - decay_after)
#         T = length(data) - 1
#         for t = 1:T
#             push!(inputs, copy(data[t])) # why copy here? there is no overwriting in AutoGrad.
#             push!(targets, copy(data[t + 1]))
#             if (t % sequence_length == 0) || t == T
#                 loss_count[1] += loss(w, inputs, targets; pdrop=pdrop)
#                 loss_count[2] += length(inputs)
#                 g = gradfun(w, inputs, targets; pdrop=pdrop)
#                 gn = (gclip > 0 ? gnorm(g) : 0)
#                 gscale = (gn > gclip > 0 ? (gclip / gn) : 1)
#                 for p in keys(w)
#                     axpy!(-lr * gscale, g[p], w[p])
#                 end
#                 empty!(inputs)
#                 empty!(targets)
#             end
#             if t % 1000 == 0
#                 elapsed_time = time() - start_time
#                 @printf(STDERR, "Epoch: %d, t: %d/%d, Loss: %.6f, LR: %.6f, Time: %.6f\n",
#                         epoch, t, T, loss_count[1] / loss_count[2], lr, elapsed_time)
#             end
#         end
#     end

#     return w, index_to_char
# end

# function loaddata(datasrc, batch_size, char_limit=0)
#     if datasrc == nothing
#         datasrc = Pkg.dir("AutoGrad/data/pg100.txt")
#     end
#     if !isfile(datasrc)
#         url = "http://www.gutenberg.org/cache/epub/100/pg100.txt"
#         download(url,datasrc)
#     end
#     stream = open(datasrc)
#     chars = Char[]
#     char_to_index = Dict{Char, Int32}()
#     while !eof(stream)
#         c = read(stream, Char)
#         get!(char_to_index, c, 1 + length(char_to_index))
#         push!(chars, c)
#         char_limit > 0 && length(chars) >= char_limit && break
#     end
#     info("Read: $(length(chars)) characters, $(length(char_to_index)) vocabulary")
#     data = minibatch(chars, char_to_index, batch_size)
#     index_to_char = Array(Char, length(char_to_index))
#     for (c, i) in char_to_index
#         index_to_char[i] = c
#     end
#     return data, chars, char_to_index, index_to_char
# end

# function generate(w, index_to_char, nchar)
#     vocab_size = length(index_to_char)
#     x = zeros(Float32, (vocab_size, 1))
#     s = nothing
#     j = 1
#     for i = 1:nchar
#         y, s = forw(w, Any[x]; state=s)
#         x[j, 1] = 0
#         j = sample(exp(y[1]))
#         x[j, 1] = 1
#         print(index_to_char[j])
#     end
#     println()
# end

# function sample(p)
#     r = rand(Float32)
#     for c = 1:length(p)
#         r -= p[c]
#         r < 0 && return c
#     end
# end

# let
#     atype = eval(parse(o[:atype]))

#     data, chars, char_to_index, index_to_char = loaddata(datasrc, batch_size, char_limit)
#     vocab_size = length(char_to_index)

#     # w = weights(o[:hidden]...; atype=atype, winit=o[:winit]) # need input/output size, load data first
#     dtrn = minibatch(xtrn, ytrn, o[:batchsize]; atype=atype)
#     dtst = minibatch(xtst, ytst, o[:batchsize]; atype=atype)
#     println((:epoch,0,:trn,accuracy(w,dtrn),:tst,accuracy(w,dtst)))
#     if o[:fast]
#         @time train(w, dtrn; lr=o[:lr], epochs=o[:epochs])
#         println((:epoch,o[:epochs],:trn,accuracy(w,dtrn),:tst,accuracy(w,dtst)))
#     else
#         @time for epoch=1:o[:epochs]
#             train(w, dtrn; lr=o[:lr], epochs=1)
#             println((:epoch,epoch,:trn,accuracy(w,dtrn),:tst,accuracy(w,dtst)))
#         end
#     end
#     return w
# end

    
# function xavier(fan_out, fan_in)
#     scale = sqrt(6 / (fan_in + fan_out))
#     return convert(Array{Float32}, 2 * scale * rand(fan_out, fan_in) - scale)
# end

# function forw(w, inputs; state=nothing, pdrop=0)
#     batch_size = size(inputs[1], 2)
#     hidden_size = size(w[:W_predict], 2)
#     if state == nothing
#         hidden = zeros(Float32, (hidden_size, batch_size))
#         cell = zeros(Float32, (hidden_size, batch_size))
#     else
#         hidden, cell = state
#     end
#     outputs = Any[]
#     for input in inputs
#         hidden, cell = lstm(w, w[:W_embedding] * input, hidden, cell)
#         pdrop > 0 && (hidden = dropout(hidden, pdrop))
#         push!(outputs, predict(w, hidden))
#     end
#     return outputs, (hidden, cell)
# end

