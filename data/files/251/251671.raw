export relu, sigmoid
import Base.tanh

for (f,df) in [(:relu,:âˆ‡relu!), (:sigmoid,:âˆ‡sigmoid!), (:tanh,:âˆ‡tanh!)]
  @eval begin
    function $f(x::Var)
      y = $f(x.value)
      df(gy) = hasgrad(x) && $df(x.value, x.grad, y, gy)
      Var(y, nothing, df, [x])
    end
  end
end

"""
Compute activation function. The supported functions are:

* `relu(x)`: rectifier linear unit
* `sigmoid(x)`
* `tanh(x)`

## ðŸ‘‰ Example
```julia
x = Var(rand(Float32,10,5))
y = relu(x)
```
"""
function relu{T}(x::Array{T})
  y = similar(x)
  @inbounds @simd for i = 1:length(x)
    y[i] = max(x[i], T(0))
  end
  y
end

function sigmoid{T}(x::Array{T})
  y = similar(x)
  @inbounds @simd for i = 1:length(x)
    y[i] = 1 / (1 + exp(-x[i]))
  end
  y
end

relu(x::CuArray) = CUDNN.activation(ActivationDesc(CUDNN_ACTIVATION_RELU), x)
sigmoid(x::CuArray) = CUDNN.activation(ActivationDesc(CUDNN_ACTIVATION_SIGMOID), x)
tanh(x::CuArray) = CUDNN.activation(ActivationDesc(CUDNN_ACTIVATION_TANH), x)

function âˆ‡relu!{T}(x::Array{T}, gx::Array{T}, y::Array{T}, gy::Array{T})
  @inbounds @simd for i = 1:length(x)
    gx[i] += ifelse(x[i]>T(0), gy[i], T(0))
  end
end

function âˆ‡sigmoid!{T}(x::Array{T}, gx::Array{T}, y::Array{T}, gy::Array{T})
  @inbounds @simd for i = 1:length(x)
    gx[i] += gy[i] * y[i] * (T(1) - y[i])
  end
end

function âˆ‡tanh!{T}(x::Array{T}, gx::Array{T}, y::Array{T}, gy::Array{T})
  @inbounds @simd for i = 1:length(gx)
    gx[i] += gy[i] * (T(1) - y[i] * y[i])
  end
end

function âˆ‡relu!(x::CuArray, gx::CuArray, y::CuArray, gy::CuArray)
  CUDNN.âˆ‡activation!(CUDNN_ACTIVATION_RELU, y, dy, x, dx, beta=1.0)
end

function âˆ‡sigmoid!(x::CuArray, gx::CuArray, y::CuArray, gy::CuArray)
  CUDNN.âˆ‡activation!(CUDNN_ACTIVATION_SIGMOID, y, dy, x, dx, beta=1.0)
end

function âˆ‡tanh!(x::CuArray, gx::CuArray, y::CuArray, gy::CuArray)
  CUDNN.âˆ‡activation!(CUDNN_ACTIVATION_TANH, y, dy, x, dx, beta=1.0)
end
