using XGBoost

###
# advanced: cutomsized loss function
#

dtrain = DMatrix("../data/agaricus.txt.train")
dtest = DMatrix("../data/agaricus.txt.test")


# note: for customized objective function, we leave objective as default
# note: what we are getting is margin value in prediction
# you must know what you are doing

param = ["max_depth"=>2, "eta"=>1, "silent"=>1]
watchlist  = [(dtest,"eval"), (dtrain,"train")]
num_round = 2

function logregobj(preds::Array{Float32, 1}, dtrain::DMatrix)
    labels = get_info(dtrain, "label")
    preds = 1.0 ./ (1.0 + exp(-preds))
    grad = preds - labels
    hess = preds .* (1.0-preds)
    return (grad, hess)
end

# user defined evaluation function, return a pair metric_name, result
# NOTE: when you do customized loss function, the default prediction value is margin
# this may make buildin evalution metric not function properly
# for example, we are doing logistic loss, the prediction is score before logistic transformation
# the buildin evaluation error assumes input is after logistic transformation
# Take this in mind when you use the customization, and maybe you need write customized evaluation function
function evalerror(preds::Array{Float32, 1}, dtrain::DMatrix)
    labels = get_info(dtrain, "label")
    # return a pair metric_name, result
    # since preds are margin(before logistic transformation, cutoff at 0)
    tmp = zip(preds, labels)
    cnt = 0
    for itm in tmp
        if convert(Integer, itm[1] > 0.0) != itm[2]
            cnt += 1
        end
    end
    return ("self-error", float(cnt / convert(Real, size(labels)[1])))
end

bst = xgboost(dtrain, num_round, param=param, watchlist=watchlist,
              obj=logregobj, feval=evalerror)

#### Not passed !!!!
