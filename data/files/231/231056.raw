export js_divergence

function d_kl_bern(p,q)
    ## Kulback-Leibler divergence for two Bernoulli variables with probabilities p and q
    d = 0.0
    if p > 0.0 # prevent NaN from 0.0 * -Inf
        d += p*log2(p./q)
    end
    if p < 1.0 # prevent NaN from 0.0 * -Inf
        d += (1.0-p)*log2((1.0-p)/(1.0-q))
    end
    return d
end
function d_js_bern(p1,p2)
    ## Jenson-Shannon divergence for two Bernoulli variables with probabilities p1 and p2
    m = 0.5*(p1+p2)
    return 0.5*d_kl_bern(p1,m) + 0.5*d_kl_bern(p2,m)
end

function js_divergence(x::Vector,y::Vector)
    ## Jenson-Shannon divergence between two multivariate Bernoulli variables
    dist = 0
    for i=1:length(x)
        dist += d_js_bern(x[i],y[i])
    end
    # normalize by the dimension. Result should be on interval [0,1].
    return dist / length(x)
end