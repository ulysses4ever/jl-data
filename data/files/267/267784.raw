function gradientFunction(params, storage, input_layer_size, hidden_layer_size, num_labels, X, y, lambda)

  #println("Params: $(size(params))")
  #println("Storage: $(size(storage))")
  #println("Input: $(input_layer_size)")
  #println("Hidden: $(hidden_layer_size)")
  #println("Labels: $(num_labels)")
  #println("X: $(size(X))")
  #println("y: $(size(y))")
  #println("Lambda: $(lambda)")
  # get the theta parameters from the params vector back into matricies
  Theta1 = reshape(params[1:hidden_layer_size * (input_layer_size + 1)],
                   hidden_layer_size, (input_layer_size + 1))
  Theta2 = reshape(params[1 + (hidden_layer_size * (input_layer_size + 1)):end],
                   num_labels, (hidden_layer_size + 1))

  Theta1_no_bias = Theta1[:, 2:end]
  Theta2_no_bias = Theta2[:, 2:end]

  m = size(X, 1)

  # setup return values
  J = 0.0
  Theta1_grad = zeros(size(Theta1))
  Theta2_grad = zeros(size(Theta2))

  # ---- begin: calucate cost
  # --> perform feedforward
  a_1 = [ones(m, 1) X]

  # get values for layer 2
  z_2 = a_1 * Theta1'
  a_2 = sigmoid(z_2)
  a_2 = [ones(m, 1) a_2]

  # get values for layer 3
  z_3 = a_2 * Theta2'
  a_3 = sigmoid(z_3)

  # --> expand labels to binary forms
  binary_labels = eye(num_labels)
  all_binary_y = binary_labels[y[:], :] # the y[:] converts the 1xnum_labels matrix to a column vector.


  # ---- end: caluclate cost

  # --- start: back propagation
  Delta_1 = zeros(size(Theta1))
  Delta_2 = zeros(size(Theta2))

  # calculate the delta for layer 3 (output).
  d_3 = a_3 - all_binary_y

  # calculate the delta for layer 2 (hidden).
  d_2 = d_3 * Theta2_no_bias .* sigmoidGradient(z_2)

  # accumulate gradients
  Delta_1 = Delta_1 + d_2' * a_1
  Delta_2 = Delta_2 + d_3' * a_2

  Theta1_grad = Delta_1/m
  Theta2_grad = Delta_2/m

  # calculate regularization term
  lambda_term = lambda/m

  # regularize gradiens
  Theta1_grad_reg = lambda_term * Theta1_no_bias
  Theta2_grad_reg = lambda_term * Theta2_no_bias

  # add and insert regularized terms back into the calculated gradients
  Theta1_grad[:, 2:end] = Theta1_grad[:, 2:end] + Theta1_grad_reg
  Theta2_grad[:, 2:end] = Theta2_grad[:, 2:end] + Theta2_grad_reg

  # --- end: back propagation
  storage[:] = [Theta1_grad[:], Theta2_grad[:]]
  grad = [Theta1_grad[:], Theta2_grad[:]]

  return grad
end

