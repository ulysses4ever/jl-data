# Modified from Mocha.jl

const width      = 32
const height     = 32
const channels   = 3
const batch_size = 10000
const n_data     = batch_size * 5 # first 50000 are good

function load_cifar_10(directory::String; data_set=:train)
	X = zeros(Float32, height, width, channels, n_data)
	y = zeros(Float32, 1, n_data)

	datasets = [("train", ["data_batch_$i.bin" for i in 1:5]),
	            ("test", ["test_batch.bin"])]

	mean_model = zeros(Float32, width, height, channels, 1)

	for (key, sources) in datasets
	    for n = 1:length(sources)
	      open(joinpath(directory, "cifar-10-batches-bin/$(sources[n])")) do f
	        println("Processing $(sources[n])...")
	        mat = readbytes(f, (1 + width*height*channels) * batch_size)
	        mat = reshape(mat, 1+width*height*channels, batch_size)

	        # random shuffle within batch
	        rp = randperm(batch_size)

	        label = convert(Array{Float32},mat[1, rp])
	        # If I divide by 256 as in the MNIST example, then
	        # training on the giving DNN gives me random
	        # performance: objective function not changing,
	        # and test performance is always 10%...
	        # The same results could be observed when
	        # running Caffe, as our HDF5 dataset is
	        # compatible with Caffe.
	        img = convert(Array{Float32},mat[2:end, rp])
	        img = reshape(img, width, height, channels, batch_size)

	        if key == "train"
	        	# only accumulate mean from the training data
				mean_model = (batch_size*mean_model + sum(img, 4)) / (n*batch_size)
	        end

	        index = (n-1)*batch_size+1:n*batch_size
	        X[:,:,:,index] = img
	        y[:,index] = label
	    end

	    # but apply mean subtraction for both training and testing data
	    println("Subtracting the mean...")
	    for n = 1:length(sources)
	      index = (n-1)*batch_size+1:n*batch_size
	      X[:,:,:,index] = X[:,:,:,index] .- mean_model
	    end
	  end
	end

	X = permutedims(X, [3, 1, 2, 4])

	n = size(y, 2)

	return (X, y)
end
