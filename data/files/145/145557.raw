using Iterators
using PyCall
@pyimport gzip
@pyimport cPickle

function preprocess_data(data)
    N = size(data[1])[1]
    X = [data[1] ones(N)]
    y = zeros(N, 10)
    for i in 1:N
        y[i, data[2][i] + 1] = 1.0
    end
    return X, y
end

function get_data()
    f = gzip.open("../../data/mnist.pkl.gz", "rb")
    (train_set, valid_set, test_set) = cPickle.load(f)
    (train_X, train_y) = preprocess_data(train_set)
    (valid_X, valid_y) = preprocess_data(valid_set)
    (test_X, test_y) = preprocess_data(test_set)

    return train_X, train_y, valid_X, valid_y, test_X, test_y
end

function initialize_weights(Ls::Array{Int64, 1})
    theta = []
    for i in 1:(length(Ls) - 1)
        push!(theta, 0.1 * randn(Ls[i] + 1, Ls[i+1]))
    end
    theta::Array{Array{Float64, 2}}
    return theta
end

function copy_theta(theta::Array{Array{Float64, 2}})
    return [copy(th) for th in theta]::Array{Array{Float64, 2}}
end

function sigmoid(z::Array{Float64, 2})
    return 1.0 ./ (1.0 + exp(-z))
end

function test(X::Array{Float64, 2}, y::Array{Float64, 2}, theta::Array{Array{Float64, 2}})
    N = size(X)[1]
    correct = 0
    for i in 1:N
        a = X[i,:]
        for th in theta
            z = a * th
            a = [sigmoid(z) 1.0]
        end
        correct += findmax(a[1, 1:end-1])[2] == findmax(y[i, :])[2]
    end

    return 1 - correct / N
end

function train(train_X::Array{Float64, 2}, train_y::Array{Float64, 2}, valid_X::Array{Float64, 2}, valid_y::Array{Float64, 2}, learning_rate::Float64, num_hidden_nodes::Int64, regularization_term::Float64, minibatch_size::Int64, epochs::Int64)
    structure = [784, num_hidden_nodes, 10]
    depth = length(structure)
    theta = initialize_weights(structure)
    prev_theta = copy_theta(theta)
    validation_error = 1.0

    stopped_early = false

    N = size(train_X)[1]
    Deltas = [zeros(th) for th in theta]::Array{Array{Float64, 2}}

    i = 1
    epoch = 0
    while epoch < epochs
        As = []
        As::Array{Array{Float64, 2}}
        push!(As, train_X[i,:]) # TODO slow
        for th in theta
            z = As[end] * th
            push!(As, [sigmoid(z) 1.0]) # TODO slow
        end
        delta = (As[end][1, 1:end-1] - train_y[i, :]) # For cross-entropy cost function
        #delta = (As[end][1, 1:end-1] - train_y[i, :]) .* As[end][1, 1:end-1] .* (1-As[end][1, 1:end-1]) # For quadratic cost function
        #delta = (As[end][1, 1:end-1] - train_y[i, :]) # For linear activation function in the final layer, with quadratic cost function
        for j in 0:(depth - 2)
            Deltas[end - j] += As[end - 1 - j]' * delta # TODO slow
            delta = ((delta * theta[end - j]') .* As[end - 1 - j] .* (1 - As[end - 1 - j]))[1, 1:end - 1] # TODO slow, broadcast
        end

        i += 1

        if mod(i, minibatch_size) == 0 # minibatch
            for l in 1:(depth-1)
                # Regularization
                Delta = (1/minibatch_size) * Deltas[l] 
                Delta[1:end-1, :] += regularization_term * minibatch_size/N * theta[l][1:end-1, :] # No reg. on thresholds. TODO slow
                theta[l] -= learning_rate * Delta
            end
            Deltas = [zeros(th) for th in theta]::Array{Array{Float64, 2}}
        end

        if i > N
            i = mod(i, N) + 1
            epoch += 1

            if mod(epoch, 5) == 0
                new_validation_error = test(valid_X, valid_y, theta)
                println("Epoch: ", epoch, ". Validation error: ", new_validation_error)
                if new_validation_error > validation_error # early stopping
                    println("Validation error increased. Early stopping.")
                    stopped_early = true
                    break
                end
                prev_theta = copy_theta(theta)
                validation_error = new_validation_error
            end
        end

    end

    if !stopped_early
        prev_theta = copy_theta(theta)
    end

    return (prev_theta, validation_error)
end

function run()

    (train_X, train_y, valid_X, valid_y, test_X, test_y) = get_data()

    # Parameters to tune
    learning_rate_list = [0.01, 0.1, 1.0]
    num_hidden_nodes_list = [20, 50, 100]
    regularization_term_list = [0.0, 0.0001, 0.001, 0.01]

    # Other parameters
    minibatch_size = 50
    epochs = 30

    best_validation_error = 1.0
    best_theta = []
    best_learning_rate = 1.0
    best_num_hidden_nodes = 10
    best_regularization_term = 1.0

    parameters = product(learning_rate_list, num_hidden_nodes_list, regularization_term_list)
    for (param_i, (learning_rate, num_hidden_nodes, regularization_term)) in enumerate(parameters)
        percent_complete = div((100 * (param_i-1)), length(parameters))
        print("\n", percent_complete, "%\tLearning rate: ", learning_rate, ", hidden nodes: ", num_hidden_nodes)
        println(", regularization: ", regularization_term)

        (theta, validation_error) = train(train_X, train_y, valid_X, valid_y, learning_rate, num_hidden_nodes, regularization_term, minibatch_size, epochs)
        if validation_error < best_validation_error
            best_validation_error = validation_error
            best_theta = [copy(th) for th in theta]::Array{Array{Float64, 2}}
            best_learning_rate = learning_rate
            best_num_hidden_nodes = num_hidden_nodes
            best_regularization_term = regularization_term
        end
    end

    println("\n\nCOMPLETE\n\nBest validation error: ", best_validation_error)
    test_error = test(test_X, test_y, best_theta)
    println("Test error: ", test_error)
    println("\nBest parameters:")
    print("learning_rate: ", best_learning_rate, ", hidden nodes: ", best_num_hidden_nodes)
    println(", regularization: ", best_regularization_term)
end

run()

