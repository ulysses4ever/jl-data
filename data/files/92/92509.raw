# parallel communication primatives

# MPI tags
global const TAG_FACE = 1  # exchanging face data
global const TAG_ELEMENT = 2  # exchanging element data

@doc """
### Utils.exchangeFaceData
  
  This function performs communications with peer processes, sending and 
  receiving data fro the shared faces.  Non-blocking sends and receives
  are posted and may or may not be waited on depending on the keyword
  arguments.  The Requests generated by the sends and receives are stored in
  mesh.recv_reqs and mesh.send_reqs.  If the Requests are waited on, the
  Status objects are stored in mesh.send_stats and mesh.recv_stats

  Inputs/Outputs:
    mesh:  an AbstractMehs
    opts:  options dictonary
    in_data: an array of arrays, where the number of arrays is the number
             of peers processes and the length of each array is the number of
             faces shared with that peer.  Tese arrays contain the data to be
             send to the other processes
    out_data: same as in_data, except the data received from the peer processes
              will be stored here

  Keyword arguments:
    tag: the MPI tag, defaults to TAG_FACE
    wait: whether or not to wait on the Request objects, default false

  Aliasing Restrictions:  none of the arrays can alias each other
"""->
function exchangeFaceData{T}(mesh::AbstractMesh, opts, 
                         in_data::Array{Array{T}, 1}, 
                         out_data::Array{Array{T}, 1}, tag=TAG_FACE,
                         wait=false)
# post sends and receives for face data exchange

  for i=1:mesh.npeers
    peer_i = mesh.peer_parts[i]
    send_buff = in_data[i]
    recv_buff = out_data[i]
    mesh.recv_reqs[i] = MPI.Irecv!(recv_buff, peer_i, tag, mesh.comm)
    mesh.send_reqs[i] = MPI.Isend(send_buff, peer_i, tag, mesh.comm)
  end

  if wait
    mesh.recv_stats = MPI.waitall!(mesh.recv_reqs)
    mesh.send_stats = MPI.waitall!(mesh.send_reqs)
  end

  return nothing
end


@doc """
### Utils.verifyCommunication

  This function checks the data provided by the Status object to verify a 
  communication completed successfully.  The sender's rank and the number of
  elements is checked agains the expected sender and the buffer size

  Inputs:
    mesh: an AbstractMesh
    opts: options dictonary
    buff: the buffer
    peer: the expected sender
    stat: the Status object
"""->
function verifyCommunication{T}(mesh::AbstractMesh, opts, buff::Array{T}, peer::Integer, stat::MPI.Status)
# verify a communication occured correctly by checking the fields of the 
# Status object
# if the Status came from a send, then peer should be comm_rank ?
  sender = getsource(stat)
  @assert sender == peer

  ndata = MPI.Get_count(stat, T)
  @assert ndata == length(buff)

  return nothing
end

@doc """
### Utils.getSendData

  This function interpolates the data that will be sent to peer processes
  and puts it into a buffer array.  This function waits for mesh.send_reqs
  to finish before overwriting the buffer.

  Inputs:
    mesh:: an AbstractDGMesh
    opts: options dictonary
    q: a 3D array numDofPerNode x numNodesPerElement x numEl holding the 
        original copy of the data
    interfaces: an array of Boundary types (not Interface) that describe
                the element and face owned by this process

  Inputs/Outputs:
    buff: array numDofPerNode x numNodesPerFace x length(interfaces) to put
         the resulting data into

  Aliasing restrictions: all bets are off if q and buff alias
"""->
function getSendData{T}(mesh::AbstractDGMesh, opts, q::AbstractArray{T, 3}, 
                    interfaces::Array{Boundary, 1}, buff::AbstractArray{T, 3})

# get data out of the q array (which must be ndofPerNode x numNodesPerElement 
# x numEl), interpolate it to the shared interfaces, and store to buff

  @assert mesh.isInterpolated

  # wait for the previous send to complete before overwritting the
  # buffer
  MPI.waitall!(mesh.send_reqs)
  boundaryinterpolate!(mesh.sbpface, interfaces, q, buff)

  return nothing
end

