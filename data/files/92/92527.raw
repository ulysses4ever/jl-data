# parallel communication primatives

# MPI tags
global const TAG_FACE = 1  # exchanging face data
global const TAG_ELEMENT = 2  # exchanging element data

function initMPIStructures(mesh::AbstractMesh, opts)

  # set all requests to REQUEST_NULL, Status to match
  # the MPI standard requires a Wait on a REQUEST_NULL to return immediately
  for i=1:mesh.npeers
    mesh.send_reqs[i] = MPI.REQUEST_NULL
    mesh.recv_reqs[i] = MPI.REQUEST_NULL
    mesh.send_stats[i] = MPI.Wait!(mesh.send_reqs[i])
    mesh.recv_stats[i] = MPI.Wait!(mesh.recv_reqs[i])
    # do not wait on these requests
    mesh.recv_waited[i] = true 
    mesh.send_waited[i] = true
  end

  return nothing
end


@doc """
### Utils.exchangeFaceData
  
  This function performs communications with peer processes, sending and 
  receiving data from the shared faces.  Non-blocking sends and receives
  are posted and may or may not be waited on depending on the keyword
  arguments.  The Requests generated by the sends and receives are stored in
  mesh.recv_reqs and mesh.send_reqs.  If the Requests are waited on, the
  Status objects are stored in mesh.send_stats and mesh.recv_stats

  Inputs/Outputs:
    mesh:  an AbstractMehs
    opts:  options dictonary
    send_data: an array of arrays, where the number of arrays is the number
             of peers processes and the length of each array is the number of
             faces shared with that peer.  Tese arrays contain the data to be
             send to the other processes
    recv_data: same as in_data, except the data received from the peer processes
              will be stored here

  Keyword arguments:
    tag: the MPI tag, defaults to TAG_FACE
    wait: whether or not to wait on the Request objects, default false

  Aliasing Restrictions:  none of the arrays can alias each other
"""->
function exchangeFaceData{T, N}(mesh::AbstractMesh, opts, 
                         send_data::Array{Array{T, N}, 1}, 
                         recv_data::Array{Array{T, N}, 1}; tag=TAG_FACE,
                         wait=false)
# post sends and receives for face data exchange

  for i=1:mesh.npeers
    peer_i = mesh.peer_parts[i]
    send_buff = send_data[i]
    recv_buff = recv_data[i]
    mesh.recv_reqs[i] = MPI.Irecv!(recv_buff, peer_i, tag, mesh.comm)
    mesh.recv_waited[i] = false
    
    if !(mesh.send_waited[i])
      mesh.send_stats[i] = MPI.Wait!(mesh.send_reqs[i])
      mesh.send_waited[i] = true
    end
    mesh.send_reqs[i] = MPI.Isend(send_buff, peer_i, tag, mesh.comm)
    mesh.send_waited[i] = false
  end

  if wait
    mesh.recv_stats = MPI.waitall!(mesh.recv_reqs)
    fill!(mesh.recv_waited, true)
    mesh.send_stats = MPI.waitall!(mesh.send_reqs)
    fill!(mesh.send_waited, true)
  end

  return nothing
end

@doc """
### Utils.exchangeElementData

  This function posts the sends and receives for the exchange of ghost 
  element data.  It waits for the previous send request to finish before
  posting the new send.  It does not wait for the previous receive.

  Inputs:
    mesh: a mesh object
    opts: options dictonary
    q: the 3D array of data, from which the data for the element to be 
       sent to other processes will be picked out

  Inputs/Outputs:
    send_buff: array of arrays to copy the send data into.  There must be 
               mesh.npeers arrays, each size(q, 1) x size(q, 2) x number of
               elements on the local side of the interface
    recv_buff: array of arrays to put the received data into.  There must be
               mesh.npeers array, each size(q,1 ) x size(q, 2) x number of
               elements on the other side of the interface

  Keyword Args
    tag=TAG_ELEMENT: MPI Tag to use
    wait=false: whether not wait for all communications to finish before
                exiting

  Aliasing: no aliasing allowed
"""->
function exchangeElementData{T, N}(mesh::AbstractMesh, opts, q::Abstract3DArray,
                                   send_buff::Array{Array{T, N}, 1},
                                   recv_buff::Array{Array{T, N}, 1}, 
                                   f::IO=STDOUT;
                                   tag=TAG_ELEMENT, wait=false)

#  println(f, "----- Entered exchangeElementData -----")
  # post recieves
  for i=1:mesh.npeers
    peer_i = mesh.peer_parts[i]
    recv_buff_i = recv_buff[i]
    mesh.recv_reqs[i] = MPI.Irecv!(recv_buff_i, peer_i, tag, mesh.comm)
    mesh.recv_waited[i] = false
  end

  npeers = mesh.npeers
  val = sum(mesh.send_waited)
  if val != mesh.npeers && val != 0  # either all have been waited on or none
    throw(ErrorException("send requests partially waited on: $val of $npeers"))
  end

  for i=1:mesh.npeers
    # wait for these in order because doing the waitany trick doesn't work
    # these should have completed long ago, so it shouldn't be a performance
    # problem
    MPI.Wait!(mesh.send_reqs[i])
    idx = i

    # copy data into send buffer
    local_els = mesh.local_element_lists[idx]
    send_buff_i = send_buff[idx]
    for j=1:length(local_els)
      el_j = local_els[j]
      for k=1:size(q, 2)
        for p=1:size(q, 1)
          send_buff_i[p, k, j] = q[p, k, el_j]
        end
      end
    end

    # send it
    peer_i = mesh.peer_parts[idx]
    mesh.send_reqs[idx] = MPI.Isend(send_buff_i, peer_i, tag, mesh.comm)
    mesh.send_waited[idx] = false
  end

  if wait
    mesh.recv_stats = MPI.waitall!(mesh.recv_reqs)
    fill!(mesh.recv_waited, true)
    mesh.send_stats = MPI.waitall!(mesh.send_reqs)
    fill!(mesh.send_waited, true)
  end

  flush(f)

  return nothing
end


@doc """
### Utils.verifyCommunication

  This function checks the data provided by the Status object to verify a 
  communication completed successfully.  The sender's rank and the number of
  elements is checked agains the expected sender and the buffer size

  Inputs:
    mesh: an AbstractMesh
    opts: options dictonary
    buff: the buffer
    peer: the expected sender
    stat: the Status object
"""->
function verifyCommunication{T}(mesh::AbstractMesh, opts, buff::Array{T}, peer::Integer, stat::MPI.Status)
# verify a communication occured correctly by checking the fields of the 
# Status object
# if the Status came from a send, then peer should be comm_rank ?
  sender = MPI.Get_source(stat)
  @assert sender == peer

  ndata = MPI.Get_count(stat, T)
  @assert ndata == length(buff)

  return nothing
end

@doc """
### Utils.getSendData

  This function interpolates the data that will be sent to peer processes
  and puts it into a buffer array.  This function waits for the Request
  to finish before overwriting the buffer.

  Inputs:
    mesh:: an AbstractDGMesh
    opts: options dictonary
    q: a 3D array numDofPerNode x numNodesPerElement x numEl holding the 
        original copy of the data
    interfaces: an array of Boundary types (not Interface) that describe
                the element and face owned by this process
    req: an MPI.Request object corresponding to the previous send using this
         buffer
    req_waited: a Bool indicating whether the request was waited on already

  Inputs/Outputs:
    buff: array numDofPerNode x numNodesPerFace x length(interfaces) to put
         the resulting data into

  Output:
    req_waited: a Bool indicating the request was waited on (should always
                be true)

  Aliasing restrictions: all bets are off if q and buff alias
"""->
function getSendData{T}(mesh::AbstractDGMesh, opts, q::AbstractArray{T, 3}, 
                    interfaces::Array{Boundary, 1}, buff::AbstractArray{T, 3},
                    req::MPI.Request, req_waited::Bool)

# get data out of the q array (which must be ndofPerNode x numNodesPerElement 
# x numEl), interpolate it to the shared interfaces, and store to buff
  @assert mesh.isInterpolated
  # wait for the previous send to complete before overwritting the
  # buffer

  if !req_waited
    MPI.Wait!(req)
    req_waited = true
  end
#  MPI.Waitall!(mesh.send_reqs)
  boundaryinterpolate!(mesh.sbpface, interfaces, q, buff)

  return req_waited
end

@doc """
### Utils.mpi_master

  This macro introduces an if statement that causes the expression to be 
  executed only if the variable myrank is equal to zero.  myrank must exist
  in the scope of the caller

"""->
macro mpi_master(ex)
  return quote
#    println("myrank = ", esc(myrank))
    if $(esc(:(myrank == 0)))
      $(esc(ex))
    end
  end
end

@doc """
### Utils.time_all 

  This macro returns the value produced by the expression as well as 
  the execution time, the GC time, and the amount of memory allocated
"""->
macro time_all(ex)
  quote
    local stats = Base.gc_num()
    local elapsedtime = time_ns()
    local val = $(esc(ex))
    elapsedtime = time_ns() - elapsedtime
    local diff = Base.GC_Diff(Base.gc_num(), stats)
    (val, elapsedtime/1e9, diff.total_time, diff.allocd)
  end
end
