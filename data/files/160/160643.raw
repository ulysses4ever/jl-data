# Interface to fitting functions (chooses binom vs betabinom based on HMM type)
function fit!(hmm; method=:baum_welch, repeats=10, max_iter=200, max_time=180, tol=1e-4, scaling=true, Bscale=1.0)
    # Setting tol to NaN will prevent early stopping, resulting in 'max_iter' iterations
    
    # Best/highest log-likelihood value
    best_ll = [-Inf]
    best_A = fill(NaN,size(hmm.A))
    best_B = fill(NaN,size(hmm.B))
    best_p = fill(NaN,size(hmm.p))

    for r = 1:repeats
        println("Fitting new HMM from random initial conditions... ", r, " / ", repeats)
        if method == :baum_welch
            ll = baum_welch!(hmm, max_iter, max_time, tol, scaling)
        else
            error("method not recognized/implemented")
        end
        if ll[end] > best_ll[end]
            best_ll = ll
            best_A = deepcopy(hmm.A)
            best_B = deepcopy(hmm.B)
            best_p = deepcopy(hmm.p)
        end
        randomize!(hmm,Bscale)
        println() # new line
    end

    # Update hmm to have the best parameters
    hmm.A = best_A
    hmm.B = best_B
    hmm.p = best_p

    return best_ll # return convergence history
end

# Expectation-Maximization a.k.a. Baum-Welch algorithm
function baum_welch!(hmm, max_iter::Int, max_time::Number, tol::Float64, scaling::Bool)
    # convergence history of the fit, log-liklihood
    ch = (Float64)[]

    start_time = time() 

    for k = 1:max_iter
        # E step
        log_p_obs, x, g = calc_stats(hmm,scaling)
        push!(ch, log_p_obs)
        
        # Test for convergence
        if length(ch)>1 && (ch[end]-ch[end-1] < tol)
            println("\nBaum-Welch converged, stopping. Final log-likelihood:  ",log_p_obs)
            break
        end

        # Test if time limit reached
        if time() - start_time > max_time
            println("\nBaum-Welch reached maximum time allowed(",max_time,"s), giving up. Final log-likelihood:  ",log_p_obs)
            break
        end

        print("*") # print each iteration
        if (k%10) == 0
            println("\nBaum-Welch iteration ", k,", log-likelihood:  ",log_p_obs)
        end

        # M step (re-estimation)
        re_estimate!(hmm, x, g)
    end

    return ch
end

#### Binomial Fitting ####

function re_estimate!(hmm::binomial_HMM,x,g)
    # Update hmm.A 
    for i = 1:hmm.n_states
        denom = sum(g[1:end-1,i]) # Expected number of transitions from state 'i'
        for j = 1:hmm.n_states
            # Numerator is the expected number of transitions from state 'i' to 'j' 
            hmm.A[i,j] = sum(x[:,i,j]) / denom
        end
    end

    # Update hmm.B
    for i = 1:hmm.n_states
        for z = 1:hmm.n_ctxt
            ## Weighted MLE for binomial distribution
            ind = hmm.c .== z # only consider observations in nucleotide context z
            wt = g[ind,i]     # weight observations by probability of being in state 'i'

            # weighted MLE calculation: number of methylation sites / total number of sites
            hmm.B[i,z] = sum(wt .* hmm.o[ind]) ./ sum(wt .* hmm.k[ind])
        end
    end

    # Update hmm.p (initial state variables)
    hmm.p = vec(g[1,:])
end

#### BetaBinomial Fitting ####

function re_estimate!(hmm::betabinom_HMM,x,g)
    # Update hmm.A 
    for i = 1:hmm.n_states
        denom = sum(g[1:end-1,i]) # Expected number of transitions from state 'i'
        for j = 1:hmm.n_states
            # Numerator is the expected number of transitions from state 'i' to 'j' 
            hmm.A[i,j] = sum(x[:,i,j]) / denom
        end
    end

    # Update betabinomial parameters
    for i = 1:hmm.n_states
        for z = 1:hmm.n_ctxt
            ind = hmm.c .== z # only consider observations in nucleotide context z
            wt = g[ind,i]     # weight observations by probability of being in state 'i'

            x0 = squeeze(hmm.B[i,z,:],(1,2)) # initial guess

            # TODO: Use method of moments to get a rough estimate of betabinomial params
            #wtn = wt / sum(wt) # normalize weights 
            #m1 = sum(wtn*hmm.o[ind])/sum(wtn*hmm.k[ind])       # sample first moment
            #m2 = sum(wtn*hmm.o[ind].^2)/sum(wtn*hmm.k[ind].^2) # sample second moment

            # Use Optim.jl to find maximum likelihood betabinomial parameters
            f = create_bb_objective_func(hmm.k[ind],hmm.o[ind],wt)
            g! = create_bb_gradient_func(hmm.k[ind],hmm.o[ind],wt)
            h! = create_bb_hessian_func(hmm.k[ind],hmm.o[ind],wt)

            #hmm.B[i,z,:] = optimize(f, g!, h!, x0, method = :newton)
            #result = optimize(f, x0, method = :nelder_mead)#, autodiff=true)
            lb = [0.0,0.0]
            ub = [5e2,5e2] # prevent overflow
            df = Optim.autodiff(f, Float64, 2)
            result = fminbox(df, x0, lb, ub)
            hmm.B[i,z,:] = result.minimum
        end
    end

    # Update hmm.p (initial state variables)
    hmm.p = vec(g[1,:])
end

